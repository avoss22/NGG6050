{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPME5o4ujI3cw3iMMq4STwE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avoss22/NGG6050/blob/main/QNC_Model_Fitting_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QNC Fit Model to Data Exercise (LATER Data)\n",
        "\n",
        "## Anna Voss | Due: 10-11-23\n",
        "\n",
        "## QNC Course Fall 2023"
      ],
      "metadata": {
        "id": "q68bCJh6Jk0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shoutout ChatGPT for translating the MatLab code in Josh's tutorials to Python: https://chat.openai.com/share/31d43f71-f845-4465-805e-c8af4a099c82"
      ],
      "metadata": {
        "id": "qd7IoiV4KDRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXVYrZp07Wvl",
        "outputId": "661dee7e-391b-4523-bccb-c9ad819c4d96"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "I4aLjuJi51d2"
      },
      "outputs": [],
      "source": [
        "#Define the function later_getData\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy.io\n",
        "\n",
        "def later_getData(subjectTag='', dataDirectory='', expressCutoff=0.0):\n",
        "    # Data directory\n",
        "    dataDirectory = os.path.expanduser(dataDirectory)\n",
        "\n",
        "    # Load the data from the subject\n",
        "    data = scipy.io.loadmat(os.path.join(dataDirectory, 'data_mgl', 'F', subjectTag + '_RT.mat'))\n",
        "\n",
        "    # Extract relevant variables from the loaded data\n",
        "    decisionSum = data['decisionSum'][0]\n",
        "    labelSum = data['labelSum'][0]\n",
        "    numdirSum = data['numdirSum'][0]\n",
        "    percorrSum = data['percorrSum'][0]\n",
        "    tRxnSum = data['tRxnSum'][0]\n",
        "\n",
        "    # Define selection criteria\n",
        "    Ltrials = (percorrSum == 1) & (tRxnSum > expressCutoff) & (tRxnSum < 1.2)\n",
        "\n",
        "    # Create four data sets\n",
        "    C_L_0 = tRxnSum[Ltrials & (numdirSum == -1) & (labelSum == 1)]\n",
        "    C_L_1_plus = tRxnSum[Ltrials & (numdirSum == -1) & (labelSum != 1)]\n",
        "    C_R_0 = tRxnSum[Ltrials & (numdirSum == 1) & (labelSum == 1)]\n",
        "    C_R_1_plus = tRxnSum[Ltrials & (numdirSum == 1) & (labelSum != 1)]\n",
        "\n",
        "    data_ = [C_L_0, C_L_1_plus, C_R_0, C_R_1_plus]\n",
        "\n",
        "    labels_ = ['Left Choice, CP', 'Left Choice, No CP', 'Right Choice, CP', 'Right Choice, No CP']\n",
        "\n",
        "    return data_, labels_\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Get the data\n",
        "\n",
        "Use this code to get a data set (array of RTs from a single condition) to fit, already preprocessed to include correct trials only and removeoutliers (including express saccades). See later_getData for details"
      ],
      "metadata": {
        "id": "2HRQOPd79End"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the later_getData function with the appropriate arguments\n",
        "data = later_getData(subjectTag='FT', dataDirectory='/content/gdrive/MyDrive/LATERdata', expressCutoff=0.2)\n",
        "\n",
        "# Extract the first element of the data list as RTs\n",
        "RTs = data[0]\n",
        "\n",
        "# There's no need to clear data as Python handles memory management automatically"
      ],
      "metadata": {
        "id": "nVO1_Wwz8-Kc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the objective function\n",
        "\n",
        "The objective function typically defines the error that you want to minimize between your data and the model predictions. A common objective function is the negative of the sum of the log-likelihoods of the data, given the model parameters. To unpack that for the LATER model:\n",
        "1. For each data point (RT from a single trial, in this case) and given a set of model parameters, compute the probability of the data, given the model (i.e., the likelihood)\n",
        "2. Take the logarithm\n",
        "3. Sum all these log-likelihoods from all the data points\n",
        "4. Take the negative, because we want to find the minimum (thus corresponding to the maximum likelihood)"
      ],
      "metadata": {
        "id": "tLP8Wy1XAVAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm  # You may need to adjust the distribution based on your model\n",
        "\n",
        "def later_model_objective(params, data):\n",
        "    \"\"\"\n",
        "    Compute the negative log-likelihood of the data given the model parameters.\n",
        "\n",
        "    Parameters:\n",
        "    - params (list or array): Model parameters to be optimized.\n",
        "    - data (array): Observed reaction times (RTs) from experimental data.\n",
        "\n",
        "    Returns:\n",
        "    - neg_log_likelihood (float): Negative log-likelihood of the data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract model parameters\n",
        "    # You would define your specific model parameters here based on your LATER model\n",
        "    # Example parameters: mu (mean), sigma (standard deviation), etc.\n",
        "    mu, sigma = params\n",
        "\n",
        "    # Compute the negative log-likelihood\n",
        "    log_likelihood = 0.0\n",
        "\n",
        "    for rt in data:\n",
        "        # Calculate the probability density function (PDF) of the normal distribution\n",
        "        pdf = norm.pdf(rt, loc=mu, scale=sigma)\n",
        "\n",
        "        # Add the log-likelihood of this data point to the total log-likelihood\n",
        "        log_likelihood += np.log(pdf)\n",
        "\n",
        "    # Take the negative log-likelihood, because we want to minimize it\n",
        "    neg_log_likelihood = -log_likelihood\n",
        "\n",
        "    return neg_log_likelihood\n"
      ],
      "metadata": {
        "id": "hgePLwgSATxb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define initial conditions\n",
        "\n",
        "For the actual fitting, we will use fmincon (https://www.mathworks.com/help/optim/ug/fmincon.html), which is \"function minimization with constraints.\" This function allows for constraints that include upper and lower bounds on the parameters. So here we define those bounds, along with the initial values.\n",
        "\n",
        "We'll use fairly arbitrary values for the lower and upper bounds, but we should pick the initial values more judiciously. HINT: Recall that the muR and deltaS should be strongly related to empirical summary statistics of `the (reciprocal) RT distribution."
      ],
      "metadata": {
        "id": "ObuL0y5MBwiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the lower and upper bounds\n",
        "lowerBounds = [0.001, 0.001]\n",
        "upperBounds = [1000, 1000]\n",
        "\n",
        "# Calculate initial values based on empirical summary statistics (you should replace these with appropriate values)\n",
        "# For example, you can use the mean and standard deviation of the data\n",
        "mean_RT = np.mean(data)  # Replace 'data' with your actual RT data\n",
        "std_RT = np.std(data)  # Replace 'data' with your actual RT data\n",
        "\n",
        "# Set initial values\n",
        "initial_values = [mean_RT, std_RT]  # Replace with your calculated values\n",
        "\n",
        "# Ensure initial values are within bounds\n",
        "initial_values = np.maximum(lowerBounds, np.minimum(upperBounds, initial_values))\n",
        "\n",
        "# Now you can use initial_values as the initial guess for your optimization\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "-2wkF9jaB85C",
        "outputId": "c2cc746c-fbc1-4a1c-8023-0e842dbef946"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ec919193be52>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Calculate initial values based on empirical summary statistics (you should replace these with appropriate values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# For example, you can use the mean and standard deviation of the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmean_RT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_array\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace 'data' with your actual RT data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mstd_RT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace 'data' with your actual RT data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3430\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3432\u001b[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[1;32m   3433\u001b[0m                           out=out, **kwargs)\n\u001b[1;32m   3434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         ret = um.true_divide(\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (78,) (92,) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Josh: My BFF ChatGPT did well in translating MatLab to Python up to this point. I had to mess around with the getData function at the beginning, so maybe this is why it doesn't like the shape of my matrix, and I'm not getting all of the metadata I need in order to compute initial values. I am stumped here."
      ],
      "metadata": {
        "id": "z7HH0NyKI7y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Run the fits\n",
        "\n",
        "We will be using GlobalSearch. The general advantage of this approach is to avoid local minima; for details, see: https://www.mathworks.com/help/gads/how-globalsearch-and-multistart-work.html\n",
        "\n",
        "These options seem to work well, but I don't have a stronger rationale for using them. See the Matlab documentation if you really want to dive in and understand them, and let me know if you find better settings!\n",
        "opts = optimoptions(@fmincon,    ... % \"function minimization with constraints\"\n",
        "   'Algorithm',   'active-set',  ...\n",
        "   'MaxIter',     3000,          ...\n",
        "   'MaxFunEvals', 3000);\n",
        "\n",
        "Definine the \"optimization problem\" using variables defined above\n",
        "problem = createOptimProblem('fmincon',    ...\n",
        "    'objective',   laterErrFcn,     ... % Use the objective function\n",
        "    'x0',          initialValues,   ... % Initial conditions\n",
        "    'lb',          lowerBounds,     ... % Parameter lower bounds\n",
        "    'ub',          upperBounds,     ... % Parameter upper bounds\n",
        "    'options',     opts);                % Options defined above\n",
        "\n",
        "Create a GlobalSearch object\n",
        "gs = GlobalSearch;\n",
        "   \n",
        "Run it, returning the best-fitting parameter values and the negative-log-likelihood returned by the objective function\n",
        "[fits(ii,:), nllk] = run(gs,problem);"
      ],
      "metadata": {
        "id": "gDZRs3xPGnzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import Bounds, differential_evolution\n",
        "\n",
        "# Define the objective function (laterErrFcn in MATLAB)\n",
        "def objective_function(params):\n",
        "    # Calculate the negative log-likelihood using your LATER model\n",
        "    neg_log_likelihood = later_model_objective(params, data)  # Use your objective function here\n",
        "    return neg_log_likelihood\n",
        "\n",
        "# Define parameter bounds (lower and upper bounds)\n",
        "lower_bounds = [0.001, 0.001]\n",
        "upper_bounds = [1000, 1000]\n",
        "\n",
        "# Define bounds for the optimization\n",
        "bounds = Bounds(lower_bounds, upper_bounds)\n",
        "\n",
        "# Set initial conditions based on empirical statistics (replace with appropriate values)\n",
        "mean_RT = np.mean(data)  # Replace 'data' with your actual RT data\n",
        "std_RT = np.std(data)  # Replace 'data' with your actual RT data\n",
        "initial_values = [mean_RT, std_RT]  # Replace with your calculated values\n",
        "\n",
        "# Run global optimization using Differential Evolution\n",
        "result = differential_evolution(objective_function, bounds, maxiter=3000, popsize=30)\n",
        "\n",
        "# Extract the best-fitting parameter values and the minimum negative log-likelihood\n",
        "best_params = result.x\n",
        "best_neg_log_likelihood = result.fun\n",
        "\n",
        "# 'best_params' contains the best-fitting parameter values\n",
        "# 'best_neg_log_likelihood' contains the minimum negative log-likelihood\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "5DBxwbW6GnMj",
        "outputId": "8bf01c27-8ec2-4ab1-8df8-762b7b8ce78a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-229a48064c4f>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Set initial conditions based on empirical statistics (replace with appropriate values)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmean_RT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace 'data' with your actual RT data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mstd_RT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Replace 'data' with your actual RT data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0minitial_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmean_RT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_RT\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Replace with your calculated values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3430\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3432\u001b[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[1;32m   3433\u001b[0m                           out=out, **kwargs)\n\u001b[1;32m   3434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         ret = um.true_divide(\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (78,) (92,) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5. Evaluate the fits\n",
        "\n",
        "EXERCISE: How do you know if you got a reasonable answer?\n",
        "\n",
        "To evaluate if you got a reasonable answer, you could calculate mean squared errors and perform residual analysis to further determne the goodness of fit of the model to the data."
      ],
      "metadata": {
        "id": "ZiiW7IUGINQg"
      }
    }
  ]
}